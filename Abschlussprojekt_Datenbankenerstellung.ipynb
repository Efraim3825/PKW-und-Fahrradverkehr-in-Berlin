{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770b99c9-4c32-43f0-85d1-f6f11a976774",
   "metadata": {},
   "source": [
    "#  <ins>Verkehrsuntersuchung</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b1161-9c40-4cf5-891d-a02504a8a65b",
   "metadata": {},
   "source": [
    "## Initialisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2008f57f-00a6-4efe-8bbe-ed09744526dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine#, text, MetaData, Table, Column, String\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c121c0ac-b350-464e-a782-0c717a27d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_password = '' # Benutze hier dein MySQL- Passwort\n",
    "engine = create_engine('mysql+mysqlconnector://root:' + sql_password + '@localhost:3306/verkehrsprojekt')\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e5f33f-2043-497b-847f-8ea897c987da",
   "metadata": {},
   "source": [
    "## Datenuntersuchung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463ed43-f836-4f1a-a8e8-7c5661b31fe6",
   "metadata": {},
   "source": [
    "### PKW- Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f85dda-88c0-46ce-ac50-f5da1b28803f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Quelle: https://api.viz.berlin.de/daten/verkehrsdetektion?path=2018%2FMessquerschnitt+%28fahrtrichtungsbezogen%29%2F <br>\n",
    "Die Quelle ist für 2018. Selbiges gibt es auch für die anderen Jahre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339548dd-e5c3-474c-a712-5ee84918a161",
   "metadata": {},
   "source": [
    "##### Erstelle für jeden Monat/Jahr ein Dataframe zur Untersuchung. Sammle all diese Dataframes in df_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "828e4072-77e3-4ff9-9487-f8792ba7dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvgz_to_df(new_csvgz, start_date, end_date):\n",
    "    with gzip.open(new_csvgz, 'rt') as f:\n",
    "        data_temp = pd.read_csv(f, sep=';')\n",
    "    \n",
    "    # Einstellen, welche Spalten wir brauchen\n",
    "    data_temp['stunde'] = data_temp['stunde'].apply(lambda x: str(x) if len(str(x)) == 2 else '0'+str(x))\n",
    "    data_temp['timestamp'] = pd.to_datetime(data_temp['tag'] + '-' + data_temp['stunde'], format = '%d.%m.%Y-%H' )\n",
    "    data_temp = data_temp[['timestamp','mq_name','q_pkw_mq_hr']]\n",
    "\n",
    "    # Erstelle ein Dataframe, das jedes TE als eigene Spalte hat\n",
    "    station_names = pd.unique(data_temp['mq_name'])\n",
    "    reference_timestamps = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "    data = pd.DataFrame({'timestamp': reference_timestamps})\n",
    "    data = data.set_index('timestamp')\n",
    "    for string in station_names:\n",
    "        filt = ( data_temp['mq_name'] == string )\n",
    "        data_temp_te = data_temp[filt]\n",
    "        data_temp_te = data_temp_te.set_index('timestamp')\n",
    "        data_temp_timestamps = data_temp_te.index\n",
    "        present_timestamps = data_temp_timestamps[data_temp_timestamps.isin(reference_timestamps)]\n",
    "        data[string] = np.nan\n",
    "        data.loc[present_timestamps, string] = data_temp_te.loc[present_timestamps,'q_pkw_mq_hr']\n",
    "        data = data.copy() # For performance improvement\n",
    "        if string[-1] == 'n': # Manche Stationsnamen ab 01.2023 haben ein 'n' am Ende. Wir entfernen das.\n",
    "            data = data.rename(columns = {string:string[:-1]})\n",
    "            if (data.columns == string[:-1]).sum() > 1: # Manche Spaltennamen kommen jetzt doppelt vor, zum Beispiel 'TE134' in 01.2023. Dann nehmen wir den Mittelwert aus den doppelten Spalten.\n",
    "                column_temp = data[string[:-1]].mean(axis = 1)\n",
    "                data = data.drop(columns = string[:-1])\n",
    "                data[string[:-1]] = column_temp    \n",
    "                \n",
    "    # Behalte nur Zeilen, in denen ein Minimum von nicht-NaN- Werten stehen\n",
    "    minimum_notnas_per_row = 25\n",
    "    filt = ( data.notna().sum(axis=1) >= minimum_notnas_per_row )\n",
    "    data = data[filt]\n",
    "    \n",
    "    data['Durchschnitt'] = data.mean(axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f447dee1-5595-45be-a573-c35553343ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collection = []\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "for year in range(2018, 2023 +1):\n",
    "    for month in months:\n",
    "        new_csvgz = 'mq_hr_' + str(year) + '_' + month  + '.csv.gz'\n",
    "        start_date = pd.Timestamp(year=year, month=int(month), day=1, hour=0, minute=0, second=0)#str(year) + '-' + month + '-01 00:00:00'\n",
    "        end_date = pd.Timestamp(year=year, month=int(month), day=start_date.days_in_month, hour=23, minute=0, second=0)#str(year) + '-' + month + '-01 00:00:00'\n",
    "        data = csvgz_to_df(new_csvgz, start_date, end_date)\n",
    "        df_collection.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56399e-aeb3-429f-88c0-3778e1e4a7c9",
   "metadata": {},
   "source": [
    "#### Excel- Tabelle vervollständigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7269ea1b-a3d5-4d42-b10b-3f7236f76f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_district_and_neighborhood(lat, lon):\n",
    "    # Initialisiere den Nominatim-Geocoder\n",
    "    geolocator = Nominatim(user_agent = \"berlin_geocoder_2345671234\") # can use any user_agent name as far as I've understood it.\n",
    "    \n",
    "    # Koordinaten umkehren (Reverse Geocoding)\n",
    "    location = geolocator.reverse((lat, lon), exactly_one=True)\n",
    "    \n",
    "    if location and location.raw:\n",
    "        # Extrahiere die relevanten Informationen\n",
    "        address = location.raw.get('address', {})\n",
    "        district = address.get('suburb', 'Bezirk nicht gefunden')\n",
    "        borough = address.get('borough', 'Stadtteil nicht gefunden')\n",
    "        return district, borough\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def district_neighborhood_pkw(row):\n",
    "    lat, lon = row[['BREITE (WGS84)','LÄNGE (WGS84)']]\n",
    "    district, neighborhood = get_district_and_neighborhood(lat, lon)\n",
    "    return district, neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bfa889f-4c53-4a5a-ab9d-53ed019f4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle die PKW- Stammdaten- Excel mitsamt Bezirken und Stadtteilen\n",
    "df = pd.read_excel('Stammdaten_Verkehrsdetektion_2022_07_20.xlsx')\n",
    "df_sample = df[['LÄNGE (WGS84)','BREITE (WGS84)']]\n",
    "new_column_series = df_sample.apply(district_neighborhood_pkw, axis = 1)\n",
    "new_columns = new_column_series.apply(pd.Series)\n",
    "df[['Bezirk','Stadtteil']] = new_columns\n",
    "df.to_excel('Stammdaten_pkw.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49a1ff-76ec-4f4e-9d47-98763e30de14",
   "metadata": {},
   "source": [
    "#### Eintragen in eine SQL- Datenbank"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0cad24a8-1085-4c48-8f94-a42f63eef393",
   "metadata": {},
   "source": [
    "Wir erstellen ein großes Dataframe big_df, welches aus allen Dataframes für jeden Monat besteht. Wir erstellen aus big_df eine Datenbank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfa8bbb9-f43a-4237-ac42-67be1bd65fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25970"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df = pd.DataFrame()\n",
    "for data in df_collection:\n",
    "    big_df = pd.concat([big_df,data])\n",
    "big_df = big_df.sort_index(axis = 1)\n",
    "\n",
    "# Stichwort Chunking: Die Tabelle ist so groß, dass wir sie in zwei Teilen in die Datenbank laden müssen. Sonst kommen komische Fehlermeldungen :(((((\n",
    "# Part 1\n",
    "big_df_part_1 = big_df.head( big_df.shape[0] // 2 )\n",
    "big_df_part_1.to_sql('pkw_daten', engine, if_exists = 'replace', index = True) # Ich betone if_exists = 'replace'\n",
    "# Part 2\n",
    "big_df_part_2 = big_df.tail( big_df.shape[0] - (big_df.shape[0] // 2) )\n",
    "big_df_part_2.to_sql('pkw_daten', engine, if_exists = 'append', index = True) # Hier hingegen if_exists = 'append'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567199e-6717-4236-9f63-5fd5291b946d",
   "metadata": {},
   "source": [
    "### Fahrrad- Daten "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb60ce-4fbb-4803-be9c-3d5bdd8c84c2",
   "metadata": {},
   "source": [
    "#### Fahrrad- Daten einpflegen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90906eff-80c7-4ac1-96d5-b9d6d1533a28",
   "metadata": {},
   "source": [
    "Quelle: https://daten.berlin.de/datensaetze/radzahldaten-in-berlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d38a94c2-8602-4ff8-9391-3affd67c42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für 'Fahrraddaten.xlsx' auf dem sheet 'Standortdaten', erstelle die Spalten 'Bezirk' und 'Stadtteil'\n",
    "\n",
    "def district_neighborhood_fahrrad(row):\n",
    "    lat, lon = row[['Breitengrad','Längengrad']]\n",
    "    district, neighborhood = get_district_and_neighborhood(lat, lon)\n",
    "    return district, neighborhood\n",
    "\n",
    "if False:\n",
    "    df = pd.read_excel('gesamtdatei-stundenwerte.xlsx', sheet_name = 'Standortdaten')\n",
    "    df_sample = df[['Breitengrad','Längengrad']]\n",
    "    new_column_series = df_sample.apply(district_neighborhood_fahrrad, axis = 1)\n",
    "    new_columns = new_column_series.apply(pd.Series)\n",
    "    df[['Stadtteil','Bezirk']] = new_columns\n",
    "    df.to_excel('Fahrrad-Stammdaten.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e7b08ef-dfa8-4f77-b3f6-ec509751c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Erstelle für jedes sheet (d.h. jedes Jahr) einen Dataframe und konkatiniere diese Dataframes zu einem df_big\n",
    "    for year in range(2018,2023 +1):\n",
    "        globals()['df_' + str(year)] = pd.read_excel('gesamtdatei-stundenwerte.xlsx', sheet_name = 'Jahresdatei ' + str(year))\n",
    "        globals()['df_' + str(year)] = globals()['df_' + str(year)].rename(columns = {'Zählstelle        Inbetriebnahme':'timestamp'})\n",
    "    df_list = [globals()['df_' + str(year)] for year in range(2018,2023 +1)]\n",
    "    df_big = pd.concat(df_list)\n",
    "    df_big = df_big.reset_index(drop = True)\n",
    "\n",
    "    # Entferne das Datum aus den Spaltennamen. z.B.  '02-MI-JAN-N 01.04.2015' --> '02-MI-JAN-N'\n",
    "    new_col_names = [string[:-11].strip() for string in df_big.columns[1:]]\n",
    "    df_big.columns = pd.core.indexes.base.Index( list(df_big.columns[:1]) + new_col_names )\n",
    "\n",
    "    #index setzen\n",
    "    df_big = df_big.set_index('timestamp')\n",
    "\n",
    "    #Füge eine Durchschnittsspalte ein\n",
    "    df_big['Durchschnitt'] = df_big.mean(axis = 1)\n",
    "    \n",
    "    df_big.to_sql('fahrraddaten', engine, if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbc0e353-fc2e-4224-92c0-43a81c567fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehlende Timestamps: DatetimeIndex(['2019-03-31 02:00:00'], dtype='datetime64[ns]', freq='h')\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    start = pd.to_datetime('2018-01-01 00:00:00')\n",
    "    end = pd.to_datetime('2023-12-31 23:00:00')\n",
    "    expected_timestamps = pd.date_range(start=start, end=end, freq='h')\n",
    "    \n",
    "    data = df_big[['Durchschnitt']]\n",
    "    \n",
    "    # Schritt 1: Überprüfen, ob alle erwarteten Timestamps im DataFrame vorhanden sind\n",
    "    missing_timestamps = expected_timestamps.difference(data.index)\n",
    "    \n",
    "    # Schritt 2: Überprüfen, ob duplikate Werte vorhanden sind\n",
    "    duplicate_timestamps = data.index[data.index.duplicated()]\n",
    "    \n",
    "    # Ausgabe:\n",
    "    if missing_timestamps.empty and duplicate_timestamps.empty:\n",
    "        print(\"Alle Stunden im Zeitraum sind genau einmal vorhanden.\")\n",
    "    else:\n",
    "        if not missing_timestamps.empty:\n",
    "            print(f\"Fehlende Timestamps: {missing_timestamps}\")\n",
    "        if not duplicate_timestamps.empty:\n",
    "            print(f\"Duplizierte Timestamps: {duplicate_timestamps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d65d60-82cf-42af-ab6d-e6854f6073c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
